
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>Grokking</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>

    <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
    
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                Deep Networks <b>Always Grok</b> and Here is Why</br> 
                <small>
                    ArXiv 2024
                </small>
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="https://imtiazhumayun.github.io">
                          Ahmed Imtiaz Humayun
                        </a>
                        </br>Rice University
                    </li>
                    <li>
                        <a href="https://randallbalestriero.github.io/">
                            Randall Balestriero
                        </a>
                        </br>Independent
                    </li>
                    <li>
                        <a href="https://richb.rice.edu/">
                          Richard Baraniuk
                        </a>
                        </br>Rice University
                    </li>
                </ul>
            </div>
        </div>


        <div class="row">
                <div class="col-md-4 col-md-offset-4 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="#">
                            <image src="img/paper_logo.jpg" height="60px" style="
                            border-style: solid;
                            border-width: thin;
                            border-color: black;
                        ">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="#">
                            <image src="img/youtube_icon.png" height="60px">
                                <h4><strong>Video</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://github.com/AhmedImtiazPrio/grok-adversarial">
                            <image src="img/github.png" height="60px">
                                <h4><strong>Code</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="#">
                            <image src="img/colab_logo.svg" height="60px">
                                <h4><strong>Colab</strong></h4>
                            </a>
                        </li>
                    </ul>
                </div>
        </div>
        
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
<!--                 <div style="display: inline-flex;flex-wrap: inherit;justify-content: center;padding-top: 15px;padding-bottom: 15px">
                          <img src="img/cnn_cifar100_nobn.svg" alt="Cifar10-CNN" style="width: 100%; max-width: 200px; height: auto; margin-bottom: 10px;">
                          <img src="img/cnn_cifar10_nobn.svg" alt="Cifar100-CNN" style="width: 100%; max-width: 200px; height: auto; margin-bottom: 10px;">
                </div>
                <div style="display: inline-flex;flex-wrap: inherit;justify-content: center;padding-top: 15px;padding-bottom: 15px">
                          <img src="img/resnet_cifar10_nobn.svg" alt="Cifar10-Resnet" style="width: 100%; max-width: 200px; height: auto; margin-bottom: 10px;">
                          <img src="img/resnet_imagenette_nobn.svg" alt="Imagenette-Resnet" style="width: 100%; max-width: 200px; height: auto; margin-bottom: 10px;">
                </div> -->
                <div style="display: inline-flex;flex-wrap: inherit;justify-content: center;padding-top: 15px;padding-bottom: 15px">
                          <img src="img/cnn_cifar10_nobn.svg" alt="Cifar10-CNN" style="width: 100%; max-width: 50%; height: auto; margin-bottom: 10px;">
                          <img src="img/resnet_imagenette_nobn.svg" alt="Imagenette-Resnet" style="width: 100%; max-width: 50%; height: auto; margin-bottom: 10px;">
                </div>
        <!--                 <p class="text-justify">
                            Fig: <b>Deep Networks grok adversarial examples.</b> When training a Resnet18 on CIFAR10 without any controlled initialization as in (liu2022omnigrok), the network starts grokking adversarial examples generated using Projected Gradient Descent after 1e4 optimization steps (left) and attains almost equal robustness and generalization performance after 2x1e5 steps.  We see that prior to grokking, the network undergoes a phase change during training in the <it>local complexity</it>, i.e., the local density of non-linearities in the input space (right). After test accuracy converges, the network starts 'migrating' its non-linearities away from the data points, and closer to the decision boundary (in video above for MLP trained on MNIST) eventually reducing the complexity of the learned function around the data points. This increase and subsequent decrease in non-linearity is a ubiquitous phenomenon for a wide variety of networks and training settings (see paper). We see that this particular training dynamic always results in delayed generalizaion or robustness.                    
                        </p> -->
                <p class="text-justify">
                    Fig: <b>Grokking Adversarial Examples.</b> We observe that Deep Neural Networks grok adversarial examples, i.e., obtain delayed robustness, without any special intervention long after test set performance has converged. We present a novel model/data/training agnostic progress measure -- <it>local complexity</it> -- which undergoes a phase change when <it>delayed generalization</it> and/or <it>delayed robustness</it> occurs. In the figure, left is for a CNN trained on CIFAR10 and right is for a ResNet18 trained on Imagenette.
                </p>
            </div> 
        </div>     

        
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3 style="text-align: center;margin-top: 30px;margin-bottom: 30px;">
                    Abstract
                </h3>
                <p class="text-justify">
                    Grokking or delayed generalization, is a phenomenon where generalization in a Deep Neural Network (DNN) occurs long after achieving near zero training error. Previous studies have reported the occurrence of grokking in controlled settings, e.g., for transformers trained on algorithmic datasets (Power et al., 2022), or for DNNs initialized with large-norm parameters (Liu et al.,2022). We instead observe that for a large number of standard and practical settings, e.g., while training a CNN on CIFAR10, or a Resnet on Imagenette, DNNs grok adversarial examples, i.e., adversarial robustness emerges long after interpolation and/or generalization. We present a theoretically motivated explanation behind the emergence of delayed generalization and delayed robustness. We find that both phenomenon are tied, originating from a phase transition in the DNN’s input space partition geometry during training. We provide the first evidence that a migration of DNN ’linear regions’ occurs, making the function progresively linear around training samples and non-linear around the decision boundary during the latest phase of training. This migration provably induces grokking, as the emergence of a robust partition widens the linear regions around the training samples.
                </p>
                </div>
            </div>
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <div style="display: flex;justify-content: center;align-items: center;padding-top: 5%;padding-bottom: 5px/* height: 400px; */">
                            <video autoplay="" muted="" loop="" playsinline="" playbackrate="2" width="100%" style="max-height: 400px;padding-right: 5px;width: 50%;">
                               <source src="img/splinecam_mnist.mp4#t=0.001" type="video/mp4">
                            </video>
                            <img src="img/mnist_mlp_grok.png" class="img-responsive" alt="mnist adversarial img" width="100%" style="padding-left: 1%;padding-top: 3%;width: auto;max-width: 50%;max-height: 450px;">
                </div>
                <p class="text-justify">
                    Fig: <b>Grokking Visualized.</b> Every neuron in a DNN learns feature thresholds/boundaries in the input space, dividing it into active and inactive regions. In the video we visualize the analytically computed partition formed by the neuron-thresholds (black lines) of a ReLU-MLP, around three MNIST training samples (white stars) in the input space. The DNN input-output mapping is linear/affine within each convex region, colored by the norm of the affine slope. In right-top we present the Train, Test and Adversarial accuracy (robustness) and right-bottom we present the local complexity, i.e., density of non-linearities, around all training, test or random data points. We see that the network starts grokking adversarial samples as soon as a phase change occurs in the partition (left), as well as phase change in the local complexity (right-bottom) approximately after 10,000 optimization steps.
                </p>
            </div>
        </div>
        
        
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3 style="padding-bottom: 20px; text-align: center;">Local Complexity - A New Progress Measure</h3>

                <p class="text-justify">
                    Progress Measures are defined as scalars that have a causal link with the training state of a DNN. We propose local complexity as a fine-grained progress measure,
                    that at a high level, denotes how complex a DNN input-output mapping is for some local neighborhood in the input space. <it>How can we measure local complexity?</it> DNNs are continuous piecewise affine splines, therefore 
                    whenever it is trained to regress a function (could be a classification objective as well), it forms a piecewise linear spline approximation of the target function. For example,
                    suppose we are training an MLP to regress a target function \( f: \mathbb{R}^2 \rightarrow \mathbb{R} \) defined as,
<!--                     \( f(x_1,x_2) = sin(x_1) + cos(x_2) \) if \( x_1 < 0 \) else \( 0 \). -->
                </p>
                <p>                
                      \[
                      f(x_1, x_2) = 
                      \begin{cases} 
                      sin(x_1) + cos(x_2) & \text{if } x_1 < 0 \\
                      0 & \text{otherwise} 
                      \end{cases}
                      \]
                </p>
<!--                 <div style="display: inline-flex;flex-wrap: inherit;justify-content: center;padding-top: 15px;padding-bottom: 15px">
                  <img src="img/toy_spline_input.png" alt="Toy input" style="width: 100%; max-width: 50%; height: auto; margin-bottom: 10px;">
                  <img src="img/toy_spline_graph.png" alt="Toy graph" style="width: 100%; max-width: 50%; height: auto; margin-bottom: 10px;">
                </div>
                 -->
               <div style="display: flex;justify-content: center;padding-top: 15px;padding-bottom: 15px">
                  <img src="img/toy_spline.svg" alt="Toy input" style="width: 500px;max-width: 100%;height: auto;margin-bottom: -10px;margin-top: -30px;">
                </div>
                <p class="text-justify">
                    Fig: <b>Linear Regions and Local Complexity.</b> Input space partition formed by a DNN trained to regress the piecewise function defined above (left) and graph of the learned function (right). Each partition region is randomly colored, and performs a single linear operation going from the input to the output. Learned function is more complex, i.e., densely changes the linear operation being performed, where the target function is non-flat.
                </p>
                <p>
                    Since the function is non-flat only for \( x_1 < 0 \) the DNN assigns more spline knots and forms more 'linear regions' for \( x_1<0 \), just like we would expect in any spline
                    interpolation task. A higher number of linear regions or non-linearities in some input domain, therefore denotes that the learned input-output mapping is more complex within that domain.  
                </p>
            </div>
        </div>  

        
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3 style="padding-bottom: 20px; text-align: center;">Connecting Spline Theory and Mechanistic Interpretability</h3>
                This progress measure and nanda measure connection. 
                layerwise explanation.  
            </div>
        </div>
    </div>
</body>
</html>
